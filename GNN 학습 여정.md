# GNN 학습 여정: 질문과 탐구로 완성된 노트 🚀

이 문서는 GNN에 대한 첫 질문부터 시작하여, 개념의 본질을 파고들고, 실제 코드를 분석하며 얻은 깊이 있는 이해를 종합한 최종 학습 기록입니다.

---

### 1. GNN의 기초: "왜?" 라는 질문으로 시작하기

우리의 여정은 GNN이 그래프를 어떻게 숫자로 표현하는지에 대한 이해에서 시작되었습니다.

-   **기본 행렬**: **인접 행렬(A)**, **차수 행렬(D)**, **그래프 라플라시안(L)**이 GNN의 기본 재료임을 확인했습니다.

#### 💡 탐구 1: `A + I`는 단순한 트릭인가?

-   **첫 질문**: "인접 행렬에 단위 행렬을 더한 건 어떤 의미야?"
-   **탐구 과정**: 단순히 'Self-loop 추가'라는 답변을 넘어, **"원래 0인 값에 1이라는 가상의 값을 더하는 게 맞는가?"** 라는 근본적인 질문으로 나아갔습니다.
-   **얻은 통찰**: `A+I`는 그래프의 '구조'를 왜곡하는 것이 아니라, GNN의 정보 취합 과정에서 **"이웃뿐만 아니라 내 자신의 정보도 고려하라"**고 알려주는 **'알고리즘적 설계'**임을 이해했습니다. 이는 **'데이터의 진실'**과 **'학습을 위한 레시피'**를 구분하는 중요한 관점이었습니다.

#### 💡 탐구 2: 정규화는 중요한 특징을 희석시키지 않는가?

-   **다음 질문**: "차수가 큰 노드는 중요한데, 정규화를 하면 그 특징이 희미해지는 것 아니야?"
-   **탐구 과정**: 이는 GCN의 본질적인 한계를 꿰뚫는 예리한 질문이었습니다.
-   **얻은 통찰**: 정규화는 '정보의 폭발'로 인한 학습 불안정성을 막기 위한 **트레이드오프**입니다. 이 질문 덕분에 우리는 GCN이 모든 이웃을 동등하게 취급하는 한계를 명확히 인지했고, 이는 자연스럽게 **GAT**의 필요성으로 이어졌습니다.

---

### 2. GNN의 핵심 원리: 정보의 흐름 파헤치기 🧠

> "이웃의 정보로 나를 표현한다"는 GNN의 대원칙이 어떤 수식으로 구현되는지 깊이 탐구했습니다.

-   **핵심 수식**: `$H^{(l+1)} = \sigma(\hat{A} H^{(l)} W^{(l)})$`
-   **질문**: "'선형 변환하여 의미 있는 임베딩 벡터 차원으로 보낸다'는 건 무슨 의미일까?"
-   **탐구 과정**: 위 수식을 두 단계로 나누어 분석했습니다.
    1.  **정보 취합 (`ÂH`)**: 이웃과 나의 정보를 모으는 '재료 준비' 단계. 여기서 **"특징이 0인 벡터는 어떻게 계산되는지"**에 대한 궁금증도 해결했습니다.
    2.  **선형 변환 (`...W`)**: 모인 정보를 신경망 레이어(`W`)로 요리하여 **'의미 있는' 새로운 특징을 학습하고, 원하는 차원으로 변환**하는 '요리' 단계임을 이해했습니다.

---

### 3. 주요 GNN 아키텍처: 문제의식과 해결의 역사

-   **GCN**: 기본 모델이지만, 모든 이웃을 동등하게 취급하여 중요한 정보가 희석될 수 있다는 한계를 가집니다.
-   **GAT**: 바로 **"중요한 노드 특징이 희미해진다"는 우리의 의문에 대한 직접적인 해답**이 되는 모델입니다. 어텐션 메커니즘을 통해 어떤 이웃에 더 '집중'할지를 스스로 학습합니다.
-   **GraphSAGE**: 샘플링과 다양한 집계 함수를 통해 확장성과 귀납적 학습 능력을 확보한 모델입니다.

---

### 4. 실전 학습: 코드는 정직하다 💻

> **질문**: "이 코드가 '연결된 노드는 가깝게, 먼 노드는 멀게'라는 취지에 맞게 수행되고 있는가?"

-   **탐구 과정**: 실제 GCN 학습 코드를 한 줄씩 분석하며, 이론이 코드로 어떻게 구현되는지 확인했습니다.
-   **얻은 통찰**: 코드의 로직은 전반적으로 맞았지만, 손실 함수에서 `sigmoid`의 출력(0~1)과 `target`(-1, 1)의 범위가 불일치하는 **결정적인 문제점을 발견**했습니다. 이는 이론만으로는 알기 어려운, 실제 구현을 통해 얻은 귀중한 학습 경험이었습니다.
-   **최종 이해**: 학습 결과로 나온 임베딩 행렬이 바로 **"그래프의 관계가 좌표로 압축된 새로운 특징 행렬"** 이라는 것을 확인하며, GNN의 입력과 출력을 완벽히 이해하게 되었습니다.

---

### 5. GNN의 활용과 확장: 그래서 무엇을 할 수 있는가?

> **질문**: "이렇게 학습시키면 어떻게 해당 모델을 활용할 수 있어?"

-   **탐구 과정**: GNN의 최종 결과물인 노드 임베딩의 활용 방안을 모색했습니다.
-   **얻은 통찰**:
    1.  **연결 예측 (Link Prediction)**: 노드 간 유사도 점수로 친구나 상품을 추천.
    2.  **다운스트림 과제 (Downstream Tasks)**: 임베딩 자체를 특징으로 사용해 **노드 분류, 커뮤니티 탐지** 등 다양한 문제 해결.

---
---

## 부록: GNN의 기반이 되는 머신러닝 지식들 📚

GNN을 더 깊이 이해하기 위해 우리가 함께 탐구했던 주변 머신러닝 개념들을 상세히 정리합니다.

### 가. 인코딩 vs. 임베딩: "번역"에도 종류가 있다

-   **인코딩 (Encoding)**: **상위 개념**. 사람이 사용하는 데이터(문자, 범주 등)를 기계가 이해할 수 있는 **숫자 형식으로 변환하는 모든 과정**을 의미합니다. 일종의 '기계어 번역' 작업입니다.
-   **임베딩 (Embedding)**: **하위 개념**. 여러 인코딩 방법 중에서도, 단어와 같은 데이터의 **'의미(semantic meaning)'를 저차원의 밀집 벡터(dense vector)에 함축**시키는 특별한 방식입니다.

| 구분 | 인코딩 (Encoding) | 임베딩 (Embedding) |
| :--- | :--- | :--- |
| **핵심 목표** | 기계가 **읽을 수 있는 형태**로 변환 | 데이터의 **의미와 관계**를 벡터 공간에 표현 |
| **결과물** | 주로 희소 벡터 (e.g., 원핫 인코딩) | 저차원의 밀집 벡터 |
| **비유** | 도서관의 **도서 등록 번호** 부여 | 도서관 서가에 **비슷한 주제끼리 책 배치** |

### 나. 차원의 저주: 왜 "저차원"이 중요한가?

-   **정의**: 데이터의 차원(feature 수)이 증가할수록 모델의 성능이 오히려 저하되거나 학습이 어려워지는 현상입니다.
-   **원인**: 차원이 커질수록 데이터 공간의 부피는 기하급수적으로 커져, 데이터가 매우 희소(sparse)해지고 모든 데이터 간의 거리가 비슷해져 버립니다.
-   **GNN과의 관계**: 단어 수가 5만 개인 그래프에서 노드 특징을 원핫 인코딩하면 5만 차원의 벡터가 됩니다. 이는 **차원의 저주**에 해당합니다. GNN이나 다른 모델에서 사용하는 **워드 임베딩은 이 5만 차원을 300차원 등으로 압축하여, '의미'는 보존하면서 차원의 저주를 피하는 핵심적인 해결책**입니다. 이 '저차원'은 원핫 인코딩의 차원에 비해 상대적으로 낮다는 의미입니다.

### 다. 단어를 벡터로 만드는 여정: BoW에서 Word2Vec까지

-   **1. Bag-of-Words (BoW)**: 가장 단순한 방법. 순서를 무시하고 문서에 어떤 단어가 몇 번 등장했는지 **빈도수**만 셉니다. "is", "the" 같은 불용어의 중요도를 높게 평가하는 한계가 있습니다.
-   **2. TF-IDF**: BoW를 개선한 방식. 한 문서에 자주 등장하면서(TF), 다른 여러 문서에서는 잘 등장하지 않는(IDF) 단어에 높은 가중치를 부여합니다. 이를 통해 문서의 **핵심 단어**를 추출할 수 있습니다.
-   **3. Word2Vec & GloVe**: 진정한 의미의 **'임베딩'** 기법들.
    -   **Word2Vec**: **"비슷한 문맥에 등장하는 단어는 비슷한 의미를 가진다"**는 아이디어로, 신경망을 통해 주변 단어를 예측하며 단어의 의미를 학습합니다.
    -   **GloVe**: 말뭉치 전체의 **'단어 동시 등장 통계'**를 활용하여 단어 벡터를 학습합니다. Word2Vec의 장점과 통계 기반 방식의 장점을 결합한 모델입니다.
