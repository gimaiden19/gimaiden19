# 강화 학습 (Reinforcement Learning)

> 인공지능이 어떤 환경 내에서 **에이전트(Agent)**가 **보상(Reward)**을 최대화하는 방향으로 행동을 학습하는 과정입니다. 기존의 지도학습, 비지도학습과 다르게, 명시적인 정답 라벨 없이 **시행착오(Trial and Error)**를 통해 학습합니다.

---

## 구성 요소

-   **에이전트 (Agent)**: 환경과 상호작용하며 학습하고 행동을 결정하는 주체입니다.
-   **환경 (Environment)**: 에이전트가 행동하고 그에 따른 보상을 받으며 상태가 변화하는 공간입니다.
-   **상태 (State)**: 특정 시점에 환경의 모든 정보를 나타내는 것으로, 에이전트가 의사 결정을 내리는 데 필요한 정보의 집합체입니다.
    -   연속적인 환경 데이터를 이산적으로 나누어 표현하기도 합니다.
-   **행동 (Action)**: 에이전트가 특정 상태에서 환경에 대해 수행하는 행위입니다.
-   **보상 (Reward)**: 에이전트가 특정 행동을 수행한 후 환경으로부터 받는 즉각적인 피드백입니다. (긍정적 보상 vs 부정적 보상)
-   **정책 (Policy)**: 특정 상태에서 에이전트가 어떤 행동을 취할지에 대한 전략 또는 규칙입니다. 최적의 정책을 학습하는 것이 강화 학습의 궁극적인 목표입니다.
-   **가치 함수 (Value Function)**: 특정 상태 또는 (상태, 행동) 쌍의 장기적인 가치를 예측하는 함수입니다.
    -   **상태 가치 함수 (V(s))**: 특정 상태 $s$에서 시작해서 정책 $\pi$를 따랐을 때 얻을 수 있는 장기적인 총 보상의 기댓값입니다.
      $$V_{\pi}(s) = E_{\pi}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s]$$
    -   **행동 가치 함수 (Q(s,a))**: 특정 상태 $s$에서 행동 $a$를 수행한 후 정책 $\pi$를 따랐을 때 얻을 수 있는 장기적인 총 보상의 기댓값입니다.
      $$Q_{\pi}(s,a) = E_{\pi}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a]$$

---

## 강화 학습의 학습 과정

> **에피소드(Episode)**: 시작 상태부터 목표에 도달하거나 실패하는 등의 종료 상태에 도달할 때까지의 (상태 - 행동 - 보상) 시퀀스

1.  **관찰**: 에이전트가 현재 환경의 상태($S_t$)를 관찰합니다.
2.  **행동 결정**: 에이전트가 자신의 정책($\pi$)에 따라 현재 상태($S_t$)에서 수행할 행동($A_t$)을 결정합니다.
3.  **환경과 상호작용**: 에이전트가 결정한 행동($A_t$)을 환경에 적용합니다.
4.  **보상 및 새로운 상태**: 환경은 에이전트의 행동($A_t$)에 대한 보상($R_{t+1}$)과 새로운 상태($S_{t+1}$)를 에이전트에게 제공합니다.
5.  **학습**: 에이전트는 받은 보상($R_{t+1}$)과 새로운 상태($S_{t+1}$)를 바탕으로 자신의 정책($\pi$)과 가치 함수를 업데이트하여 더 나은 행동을 학습합니다.

---

## 탐험 (Exploration)과 활용 (Exploitation)

-   **탐험**: 아직 시도해보지 않은 행동들을 탐색하여 더 많은 정보를 얻는 것입니다.
    -   단점: 탐험에 너무 치우치면 좋은 전략을 발견하고도 이를 활용하지 못할 수 있습니다.
-   **활용**: 현재까지 학습한 지식을 바탕으로 가장 좋은 보상을 줄 것이라고 예상되는 행동을 선택하는 것입니다.
    -   단점: 활용에 너무 치우치면 더 좋은 보상을 받을 수 있는 다른 행동을 발견하지 못하고 **지역 최적해(Local Optimum)**에 빠질 수 있습니다.

---

## 주요 알고리즘 유형

-   **가치 기반 알고리즘 (Value-based)**: 가치 함수를 학습하여 최적의 정책을 간접적으로 도출합니다.
    -   *예시: Q-러닝(Q-Learning), SARSA 등*
-   **정책 기반 알고리즘 (Policy-based)**: 정책 자체를 직접 학습하여 최적의 행동을 선택합니다.
    -   *예시: REINFORCE, Actor-Critic 등*
-   **모델 기반 알고리즘 (Model-based)**: 환경의 모델(상태 전이 확률, 보상 함수 등)을 학습하고, 이 모델을 기반으로 최적의 행동을 계획합니다.
-   **모델 프리 알고리즘 (Model-free)**: 환경의 모델을 명시적으로 학습하지 않고, 시행착오를 통해 직접 학습합니다.
    -   *예시: Q-러닝, SARSA, 정책 경사 방법(Policy Gradient Methods) 등*

---

## 응용 분야

-   **게임**: 알파고(바둑), OpenAI Five(도타 2) 등
-   **로봇 제어**: 보행 로봇, 로봇 팔 제어 등
-   **자원 관리**: 에너지 그리드 관리, 데이터 센터 냉각 시스템 최적화 등
-   **추천 시스템**: 사용자에게 개인화된 콘텐츠 추천
-   **금융**: 주식 거래 전략 최적화, 포트폴리오 관리
-   **헬스케어**: 약물 투여량 최적화, 질병 진단 및 치료 계획 수립
