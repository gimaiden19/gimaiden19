# GNN(Graph Neural Network) 학습 노트

---

## 1. GNN의 기본 원리 🧠

> GNN의 핵심 아이디어는 한마디로 **"이웃의 정보로 나를 표현한다"**입니다.

- **그래프(Graph)**: 관계를 가진 데이터. **노드(Node)**와 이를 잇는 **엣지(Edge)**로 구성됩니다.
- **메시지 전달 (Message Passing)**: GNN의 핵심 동작 원리. 각 노드가 이웃 노드들의 정보를 모아 자신의 정보를 업데이트하는 과정입니다.
    1.  **정보 수집 (Aggregation)**: 이웃 노드들의 정보(벡터)를 하나로 합칩니다.
        - `Mean`: 이웃 정보의 평균. GCN에서 사용. 안정적입니다.
        - `Sum`: 이웃 정보의 합. 정보 보존에 유리하나 스케일에 민감합니다.
        - `Max`: 이웃 정보의 최대값. 핵심 특징 추출에 효과적입니다.
    2.  **정보 업데이트 (Update)**: 수집된 이웃 정보와 자신의 기존 정보를 신경망(Neural Network)을 통해 결합하여 새로운 정보로 업데이트합니다.

---

## 2. 주요 GNN 모델 아키텍처 🏛️

### 가. GCN (Graph Convolutional Network)
- **개념**: CNN의 컨볼루션 연산을 그래프에 맞게 재해석한 GNN의 기본 모델. 이웃 정보의 **정규화된 평균**을 사용합니다.
- **특징**:
    - 간결하고 효율적이며, 스펙트럼 이론에 기반합니다.
    - **전이 학습(Transductive Learning)**: 학습 시 전체 그래프 구조를 알아야 합니다.
- **한계**:
    - **귀납적 학습(Inductive Learning) 불가**: 학습 후 새로운 노드가 추가되면 예측이 어렵습니다.
    - **확장성**: 거대 그래프에 적용하기 어렵습니다.
    - **표현력**: 모든 이웃을 동등하게 취급합니다.

### 나. GraphSAGE (Graph SAmple and aggreGatE)
- **개념**: GCN의 한계를 극복하기 위해 **샘플링(Sample)**과 **집계(Aggregate)** 방식을 개선한 모델.
- **특징**:
    - **이웃 샘플링**: 모든 이웃 대신 고정된 수의 이웃을 샘플링하여 계산 효율과 확장성을 확보합니다.
    - **학습 가능한 집계 함수**: Mean 외에 LSTM, Pooling 등 표현력 높은 집계 함수를 **직접 학습**합니다.
    - **귀납적 학습(Inductive Learning) 가능**: '정보를 집계하는 함수'를 학습하므로 새로운 노드에도 바로 적용할 수 있습니다.

### 다. GAT (Graph Attention Network)
- **개념**: "모든 이웃이 똑같이 중요하지는 않다"는 아이디어에서 출발. Transformer의 **어텐션 메커니즘**을 도입했습니다.
- **특징**:
    - **어텐션 스코어**: 노드 간의 관계 중요도를 동적으로 계산하여 가중 평균을 냅니다.
    - **높은 표현력**: 더 복잡하고 중요한 관계를 모델링할 수 있습니다.
    - **귀납적 학습(Inductive Learning) 가능**: 어텐션 계산은 지역적(Local) 연산이므로 새로운 노드에 적용 가능합니다.

---

## 3. 핵심 개념 심화 📚

### 가. 텍스트 데이터 처리
- 텍스트 자체는 특징 벡터가 없으므로, GNN 적용 전에 **BERT**와 같은 사전 학습된 언어 모델을 사용해 텍스트를 **고정된 크기의 벡터로 변환(Embedding)**하는 과정이 필요합니다. GNN은 이 임베딩 벡터를 초기 노드 특징으로 사용합니다.

### 나. 홉 (Hop)의 개념
- **구조적 홉 (Structural Hop)**: 그래프 자체의 연결 관계. 엣지로 직접 연결되면 1홉입니다. (정적인 정의)
- **연산적 홉 (Computational Hop)**: GNN이 정보를 전파하는 단위. 1-Layer GNN은 1홉, k-Layer GNN은 k홉 반경의 정보를 집계합니다. 인접 행렬의 거듭제곱($A^k$)은 k홉 경로의 수를 계산하는 것과 관련이 있습니다. (동적인 활용)

### 다. 학습 방식
- **준지도학습 (Semi-Supervised Learning)**
    - **정의**: 아주 적은 양의 **정답(Label) 데이터**와 대량의 **정답 없는 데이터**를 함께 사용해 학습하는 방식.
    - **GCN의 활용**: 그래프 구조를 이용해 소수 노드의 라벨 정보를 정답 없는 주변 노드들로 전파시키며 전체 모델의 성능을 높입니다.
- **전이 학습 (Transfer Learning)**
    - **정의**: 대규모 데이터로 미리 학습된 모델(Pre-trained Model)의 지식을 가져와, 새롭지만 관련된 문제에 적용(Fine-Tuning)하는 기법.
    - **장점**: 적은 데이터로도 높은 성능을 낼 수 있고, 학습 시간과 자원을 절약합니다.

---

## 4. GCN 논문(참고)
- **제목**: "Semi-Supervised Classification with Graph Convolutional Networks" (T. Kipf, M. Welling, ICLR 2017)
- **의의**: 복잡했던 스펙트럼 기반 GNN을 **혁신적으로 단순화**하여 GNN의 대중화를 이끈 논문.
- **링크**: [https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907)
