# GNN(Graph Neural Network) 학습 노트

---

## 1. 그래프를 표현하는 핵심 행렬 🔢

GNN이 그래프를 이해하기 위해 사용하는 세 가지 핵심 행렬입니다.

> 🧑‍🏫 **공통 예제: 4명의 소셜 네트워크**
> - **노드**: 1, 2, 3, 4
> - **엣지**: (1, 2), (1, 3), (2, 3), (2, 4), (3, 4)

### 가. 인접 행렬 (Adjacency Matrix, A)
- **정의**: 노드 간의 **직접적인 연결 관계**를 표현합니다. 연결되어 있으면 1, 아니면 0입니다.
- **의미**: GNN이 각 노드의 이웃이 누구인지를 파악하는 가장 기본적인 정보입니다.
- **예시**:
  $$
  A = \begin{pmatrix}
  0 & 1 & 1 & 0 \\
  1 & 0 & 1 & 1 \\
  1 & 1 & 0 & 1 \\
  0 & 1 & 1 & 0
  \end{pmatrix}
  $$

### 나. 차수 행렬 (Degree Matrix, D)
- **정의**: 각 노드가 몇 개의 엣지를 가졌는지(차수)를 **대각선에 표현**하는 행렬입니다.
- **의미**: 각 노드의 연결성 정도 또는 중요도를 나타내며, GNN에서 정보를 정규화(Normalization)하여 학습을 안정시키는 데 사용됩니다.
- **예시**: (노드 1,2,3,4의 차수는 각각 2,3,3,2)
  $$
  D = \begin{pmatrix}
  2 & 0 & 0 & 0 \\
  0 & 3 & 0 & 0 \\
  0 & 0 & 3 & 0 \\
  0 & 0 & 0 & 2
  \end{pmatrix}
  $$

### 다. 그래프 라플라시안 (Graph Laplacian, L)
- **정의**: **차수 행렬에서 인접 행렬을 뺀 행렬** ($L = D - A$)입니다.
- **의미**: 그래프의 구조적 특성을 종합적으로 담고 있습니다. 연결된 노드들 간의 신호(feature)가 얼마나 '매끄러운지(smooth)'를 측정하며, **스펙트럴(Spectral) GNN**의 이론적 기반이 됩니다.
- **예시**:
  $$
  L = \begin{pmatrix}
  2 & -1 & -1 & 0 \\
  -1 & 3 & -1 & -1 \\
  -1 & -1 & 3 & -1 \\
  0 & -1 & -1 & 2
  \end{pmatrix}
  $$

---

## 2. GNN의 기본 원리 🧠

> GNN의 핵심 아이디어는 한마디로 **"이웃의 정보로 나를 표현한다"**입니다.

- **메시지 전달 (Message Passing)**: GNN의 핵심 동작 원리. 각 노드가 이웃 노드들의 정보를 모아 자신의 정보를 업데이트하는 과정입니다.
    1.  **정보 수집 (Aggregation)**: 이웃 노드들의 정보(벡터)를 하나로 합칩니다.
        - `Mean`, `Sum`, `Max` 등 다양한 전략이 사용됩니다.
    2.  **정보 업데이트 (Update)**: 수집된 이웃 정보와 자신의 기존 정보를 신경망(Neural Network)을 통해 결합하여 새로운 정보로 업데이트합니다.

---

## 3. 주요 GNN 모델 아키텍처 🏛️

### 가. GCN (Graph Convolutional Network)
- **개념**: CNN의 컨볼루션 연산을 그래프에 맞게 재해석한 GNN의 기본 모델. 이웃 정보의 **정규화된 평균**을 사용합니다.
- **특징**:
    - 간결하고 효율적이며, **스펙트럴 GNN을 혁신적으로 단순화**한 모델입니다.
    - **전이 학습(Transductive Learning)**: 학습 시 전체 그래프 구조를 알아야 합니다.
- **한계**:
    - **귀납적 학습(Inductive Learning) 불가**: 학습 후 새로운 노드가 추가되면 예측이 어렵습니다.
    - **확장성**: 거대 그래프에 적용하기 어렵습니다.
    - **표현력**: 모든 이웃을 동등하게 취급합니다.

### 나. GraphSAGE (Graph SAmple and aggreGatE)
- **개념**: GCN의 한계를 극복하기 위해 **샘플링(Sample)**과 **집계(Aggregate)** 방식을 개선한 모델.
- **특징**:
    - **이웃 샘플링**: 모든 이웃 대신 고정된 수의 이웃을 샘플링하여 계산 효율과 확장성을 확보합니다.
    - **학습 가능한 집계 함수**: Mean 외에 LSTM, Pooling 등 표현력 높은 집계 함수를 **직접 학습**합니다.
    - **귀납적 학습(Inductive Learning) 가능**: '정보를 집계하는 함수'를 학습하므로 새로운 노드에도 바로 적용할 수 있습니다.

### 다. GAT (Graph Attention Network)
- **개념**: "모든 이웃이 똑같이 중요하지는 않다"는 아이디어에서 출발. Transformer의 **어텐션 메커니즘**을 도입했습니다.
- **특징**:
    - **어텐션 스코어**: 노드 간의 관계 중요도를 동적으로 계산하여 가중 평균을 냅니다.
    - **높은 표현력**: 더 복잡하고 중요한 관계를 모델링할 수 있습니다.
    - **귀납적 학습(Inductive Learning) 가능**: 어텐션 계산은 지역적(Local) 연산이므로 새로운 노드에 적용 가능합니다.

---

## 4. 핵심 개념 심화 📚

### 가. 스펙트럴 GNN과 라플라시안의 역할
- **역할**: 그래프 라플라시안은 그래프 데이터를 분석하기 쉬운 **"주파수(Frequency)" 영역으로 변환**하는 다리 역할을 합니다.
    - **저주파**: 이웃 노드끼리 비슷한 값을 갖는 상태 (Smooth)
    - **고주파**: 이웃 노드끼리 다른 값을 갖는 상태 (Sharp)
- **사용처**: 스펙트럴 GNN은 라플라시안을 이용해 **그래프 푸리에 변환**을 수행합니다. 이를 통해 노드 특징을 주파수 영역으로 보내고, 특정 주파수 대역을 강조/약화시키는 **스펙트럴 필터링**을 적용하여 정교한 특징을 학습합니다.

### 나. 텍스트 데이터 처리
- 텍스트 자체는 특징 벡터가 없으므로, GNN 적용 전에 **BERT**와 같은 사전 학습된 언어 모델을 사용해 텍스트를 **고정된 크기의 벡터로 변환(Embedding)**하는 과정이 필요합니다. GNN은 이 임베딩 벡터를 초기 노드 특징으로 사용합니다.

### 다. 학습 방식
- **준지도학습 (Semi-Supervised Learning)**
    - **정의**: 아주 적은 양의 **정답(Label) 데이터**와 대량의 **정답 없는 데이터**를 함께 사용해 학습하는 방식.
    - **GCN의 활용**: 그래프 구조를 이용해 소수 노드의 라벨 정보를 정답 없는 주변 노드들로 전파시키며 전체 모델의 성능을 높입니다.
- **전이 학습 (Transfer Learning)**
    - **정의**: 대규모 데이터로 미리 학습된 모델(Pre-trained Model)의 지식을 가져와, 새롭지만 관련된 문제에 적용(Fine-Tuning)하는 기법.
    - **장점**: 적은 데이터로도 높은 성능을 낼 수 있고, 학습 시간과 자원을 절약합니다.

---

## 5. GNN의 응용: 음식 조합 및 추천 🧑‍🍳
- **그래프 구성**:
    - **노드**: '스테이크', '와인' 등 개별 음식이나 식재료
    - **엣지**: '같은 레시피에 등장', '향미 성분 공유' 등 다양한 관계
- **학습 및 활용**:
    - GNN은 음식 관계 그래프를 학습하여 각 음식을 **임베딩 벡터**로 표현합니다.
    - 이 임베딩 공간에서 벡터 간의 거리를 계산하여 **음식을 추천**하거나, 기존에 없던 **새로운 조합을 발굴**하는 데 활용할 수 있습니다.
- **의의**: 스펙트럴 GNN의 '그래프 전체 구조를 통해 관계성을 학습한다'는 이론은 음식 조합 문제에 완벽한 통찰을 제공하며, 이를 **GCN, GAT** 등 실용적인 모델로 구현할 수 있습니다.
